{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offchart-star-GPT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXigi0eadjuiS61juDolZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riccardo247/star-GPT/blob/main/offchart_star_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tii5zEBFydn"
      },
      "source": [
        "**Download all needd packages**\n",
        "\n",
        "\n",
        "> package GPTNeo is needed with all the requirements\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qe-S3h9AKpd",
        "outputId": "1f16d632-52e5-4e1e-f910-0ee214d317d6"
      },
      "source": [
        "#@title Setup\n",
        "%tensorflow_version 2.x\n",
        "!git clone https://github.com/EleutherAI/GPTNeo\n",
        "%cd GPTNeo\n",
        "!pip3 install -q -r requirements.txt\n",
        "pretrained_model = None\n",
        "dataset = None"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GPTNeo'...\n",
            "remote: Enumerating objects: 3747, done.\u001b[K\n",
            "remote: Counting objects: 100% (138/138), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 3747 (delta 77), reused 127 (delta 69), pack-reused 3609\u001b[K\n",
            "Receiving objects: 100% (3747/3747), 1.44 MiB | 9.57 MiB/s, done.\n",
            "Resolving deltas: 100% (2167/2167), done.\n",
            "/content/GPTNeo\n",
            "\u001b[K     |████████████████████████████████| 368kB 6.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 14.2MB 222kB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 55.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 394.7MB 39kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 38.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 30.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 28.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 184kB 40.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 39.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 39.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 47.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 43.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 286kB 52.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n",
            "\u001b[?25h  Building wheel for tpunicorn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ring (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wirerope (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KgpQVa6F2K5"
      },
      "source": [
        "**Gopogle authentication** could be needed to access the Google bucket with weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pofjb285ESuf",
        "outputId": "a297a4e8-a73a-4dde-dff4-bcd76a1db2d0"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud init1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome! This command will take you through the configuration of gcloud.\n",
            "\n",
            "Settings from your current configuration [default] are:\n",
            "component_manager:\n",
            "  disable_update_check: 'True'\n",
            "compute:\n",
            "  gce_metadata_read_timeout_sec: '0'\n",
            "core:\n",
            "  account: riccardo.giacomelli@gmail.com\n",
            "\n",
            "Pick configuration to use:\n",
            " [1] Re-initialize this configuration [default] with new settings \n",
            " [2] Create a new configuration\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "Your current configuration has been set to: [default]\n",
            "\n",
            "You can skip diagnostics next time by using the following flag:\n",
            "  gcloud init --skip-diagnostics\n",
            "\n",
            "Network diagnostic detects and fixes local network connection issues.\n",
            "Reachability Check passed.\n",
            "Network diagnostic passed (1/1 checks passed).\n",
            "\n",
            "Choose the account you would like to use to perform operations for \n",
            "this configuration:\n",
            " [1] riccardo.giacomelli@gmail.com\n",
            " [2] Log in with a new account\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "You are logged in as: [riccardo.giacomelli@gmail.com].\n",
            "\n",
            "Pick cloud project to use: \n",
            " [1] bigbird-emc2\n",
            " [2] bigbird-freefly\n",
            " [3] birbird-emc2\n",
            " [4] optimum-monitor-215813\n",
            " [5] test-tpugpu\n",
            " [6] Create a new project\n",
            "Please enter numeric choice or text value (must exactly match list \n",
            "item):  2\n",
            "\n",
            "Your current project has been set to: [bigbird-freefly].\n",
            "\n",
            "Do you want to configure a default Compute Region and Zone? (Y/n)?  n\n",
            "\n",
            "Your Google Cloud SDK is configured and ready to use!\n",
            "\n",
            "* Commands that require authentication will use riccardo.giacomelli@gmail.com by default\n",
            "* Commands will reference project `bigbird-freefly` by default\n",
            "Run `gcloud help config` to learn how to change individual settings\n",
            "\n",
            "This gcloud configuration is called [default]. You can create additional configurations if you work with multiple accounts and/or projects.\n",
            "Run `gcloud topic configurations` to learn more.\n",
            "\n",
            "Some things to try next:\n",
            "\n",
            "* Run `gcloud --help` to see the Cloud Platform services you can interact with. And run `gcloud help COMMAND` to get help on any gcloud command.\n",
            "* Run `gcloud topic --help` to learn about advanced features of the SDK like arg files and output formatting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxAK-TCzGJeS"
      },
      "source": [
        "**Configuration**\n",
        "\n",
        "\n",
        "> The following are configurations files for the model and data. They do not need to be modified\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMCzeJG8Ap1P",
        "outputId": "da160f58-28e8-4db9-d90d-1464ee423d0b"
      },
      "source": [
        "%%writefile configs/GPT3_XL_bakeoff.json\n",
        "\n",
        "{\n",
        "    \"n_head\": 16,\n",
        "    \"n_vocab\": 50257,\n",
        "    \"embed_dropout\": 0,\n",
        "    \"lr\": 0.0002,\n",
        "    \"lr_decay\": \"cosine\",\n",
        "    \"warmup_steps\": 3000,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.95,\n",
        "    \"epsilon\": 1e-8,\n",
        "    \"opt_name\": \"adam\",\n",
        "    \"weight_decay\": 0,\n",
        "    \"train_batch_size\": 1,\n",
        "    \"attn_dropout\": 0,\n",
        "    \"train_steps\": 744136,\n",
        "    \"lr_decay_end\" : 300000,\n",
        "    \"eval_steps\": 20,\n",
        "    \"predict_steps\":1,\n",
        "    \"res_dropout\": 0,\n",
        "    \"eval_batch_size\": 1,\n",
        "    \"predict_batch_size\": 1,\n",
        "    \"iterations\": 100,\n",
        "    \"n_embd\": 2048,\n",
        "    \"datasets\": [[\"sst3_shortbake_label_pred\"]], \n",
        "    \"model_path\": \"gs://bigbird-freefly/neo_gpt3/gpt3_XL/the-eye.eu/public/AI/gptneo-release/all_balanced_train_b\",\n",
        "    \"n_ctx\": 2048,\n",
        "    \"n_layer\": 24,\n",
        "    \"scale_by_depth\": true,\n",
        "    \"scale_by_in\": false,\n",
        "    \"attention_types\" :  [[[\"global\", \"local\"],12]],\n",
        "    \"mesh_shape\": \"x:1,y:8\",\n",
        "    \"layout\" : \"batch:x,memory_length:y,embd:y\",\n",
        "    \"activation_function\": \"gelu\",\n",
        "    \"recompute_grad\": true,\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"tokens_per_mb_per_replica\": 2048,\n",
        "    \"precision\": \"bfloat16\",\n",
        "    \"padding_id\" : 50257,\n",
        "    \"eos_id\" : 50256\n",
        "}"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting configs/GPT3_XL_bakeoff.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQESZkbiDjBZ",
        "outputId": "81ff37fd-de61-4810-9728-11081d3661ae"
      },
      "source": [
        "%%writefile configs/dataset_configs/sst3_shortbake_label_pred.json\n",
        "{\n",
        "    \"n_vocab\": 50257,\n",
        "    \"path\": \"gs://bigbird-freefly/sst3/bakeoff_entry//sst*.tfrecords\", \n",
        "    \"eval_path\": \"gs://bigbird-freefly/sst3/dev//imbd*.tfrecords\",\n",
        "    \"tokenizer_is_pretrained\": true,\n",
        "    \"tokenizer_path\": \"gpt2\",\n",
        "    \"eos_id\" : 50256,\n",
        "    \"padding_id\": 50257\n",
        "}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting configs/dataset_configs/sst3_shortbake_label_pred.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruZbvBhRIcf8",
        "outputId": "7729cddc-054e-46e3-ae99-e3d05cd2d9a0"
      },
      "source": [
        "!gsutil cp gs://bigbird-freefly/sst3/files/sample.py ./"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bigbird-freefly/sst3/files/sample.py...\n",
            "/ [1 files][  9.1 KiB/  9.1 KiB]                                                \n",
            "Operation completed over 1 objects/9.1 KiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vzWJi_vGZFB"
      },
      "source": [
        "**Review examples**\n",
        "\n",
        "\n",
        "> The following code cell writes to a txt a review example. The text should be formatted in 3 lines as:\n",
        "\n",
        "\n",
        "> This is the review:\n",
        "...put the text of the review in this line...\n",
        "sentiment is:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkSfy4FUJmdZ",
        "outputId": "a2e968d2-eee2-479d-c4a5-f7b1d2b54faf"
      },
      "source": [
        "%%writefile review_11820-00001.txt\n",
        "This is a review:\n",
        "Enough similarities to Gymkata and Howie Long 's Firestorm that my fingernails instinctively crawled towards my long-suffering eyeballs .\n",
        "sentiment is:"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting review_11820-00001.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_K5dTBZA2Er",
        "outputId": "88b716f3-ff8c-403f-8b27-856d7f0eff39"
      },
      "source": [
        "%%writefile review_11830-00001.txt\n",
        "This is a review:\n",
        "While the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun .\n",
        "sentiment is:"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting review_11830-00001.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES59Rca6KiTU"
      },
      "source": [
        "**Sentimen prediction:**\n",
        "The follosing is running prediction for the txt file and output the result to the cell.\n",
        "\n",
        ">The first time it will take 10 minutes because it has to download the model weights. Second time will be 1-2 minutes. This notebook is just to show how it works but it is very slow!\n",
        "Output will be the input ending in sentiment like this:\n",
        "This is a review:\n",
        "While the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun .\n",
        "sentiment is: **negative**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x2Qg1OVCdWi",
        "outputId": "cc9befe7-fce4-4498-e341-39819d6d05b3"
      },
      "source": [
        "review_file ='review_11820-00001.txt'\n",
        "!python3 main.py --model GPT3_XL_bakeoff --steps_per_checkpoint 100 --tpu colab --predict --prompt $review_file"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-21 20:45:26.644600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Current step 736911\n",
            "Saving config to gs://bigbird-freefly/neo_gpt3/gpt3_XL/the-eye.eu/public/AI/gptneo-release/all_balanced_train_b\n",
            "2021-04-21 20:45:35.333094: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-21 20:45:35.334145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-21 20:45:35.346532: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-04-21 20:45:35.346619: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (945b4dd6c79b): /proc/driver/nvidia/version does not exist\n",
            "2021-04-21 20:45:37.541947: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
            "Done!\n",
            "params = defaultdict(<function fetch_model_params.<locals>.<lambda> at 0x7f17d57784d0>, {'n_head': 16, 'n_vocab': 50257, 'embed_dropout': 0, 'lr': 0.0002, 'lr_decay': 'cosine', 'warmup_steps': 3000, 'beta1': 0.9, 'beta2': 0.95, 'epsilon': 1e-08, 'opt_name': 'adam', 'weight_decay': 0, 'train_batch_size': 1, 'attn_dropout': 0, 'train_steps': 744136, 'lr_decay_end': 300000, 'eval_steps': 20, 'predict_steps': 1, 'res_dropout': 0, 'eval_batch_size': 1, 'predict_batch_size': 1, 'iterations': 100, 'n_embd': 2048, 'datasets': [['sst3_shortbake_label_pred']], 'model_path': 'gs://bigbird-freefly/neo_gpt3/gpt3_XL/the-eye.eu/public/AI/gptneo-release/all_balanced_train_b', 'n_ctx': 2048, 'n_layer': 24, 'scale_by_depth': True, 'scale_by_in': False, 'attention_types': ['global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local'], 'mesh_shape': 'x:1,y:8', 'layout': 'batch:x,memory_length:y,embd:y', 'activation_function': 'gelu', 'recompute_grad': True, 'gradient_clipping': 1.0, 'tokens_per_mb_per_replica': 2048, 'precision': 'bfloat16', 'padding_id': 50257, 'eos_id': 50256, 'dataset_configs': {'sst3_shortbake_label_pred': {'n_vocab': 50257, 'path': 'gs://bigbird-freefly/sst3/bakeoff_entry//sst*.tfrecords', 'eval_path': 'gs://bigbird-freefly/sst3/dev//imbd*.tfrecords', 'tokenizer_is_pretrained': True, 'tokenizer_path': 'gpt2', 'eos_id': 50256, 'padding_id': 50257}}, 'mlm_training': False, 'causal': True, 'num_cores': 8, 'auto_layout': False, 'auto_layout_and_mesh_shape': False, 'use_tpu': True, 'gpu_ids': ['device:GPU:0'], 'steps_per_checkpoint': 100, 'predict': True, 'model': 'GPT', 'export': False, 'sampling_use_entmax': False, 'moe_layers': None, 'slow_sampling': False})\n",
            "Using config: {'_model_dir': 'gs://bigbird-freefly/neo_gpt3/gpt3_XL/the-eye.eu/public/AI/gptneo-release/all_balanced_train_b', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.113.140.66:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.113.140.66:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.113.140.66:8470', '_evaluation_master': 'grpc://10.113.140.66:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7f17d57619d0>}\n",
            "_TPUContext: eval_on_tpu True\n",
            "Predictions generated\n",
            "Querying Tensorflow master (grpc://10.113.140.66:8470) for TPU system metadata.\n",
            "2021-04-21 20:45:39.805784: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "Initializing TPU system (master: grpc://10.113.140.66:8470) to fetch topology for model parallelism. This might take a while.\n",
            "Found TPU system:\n",
            "*** Num TPU Cores: 8\n",
            "*** Num TPU Workers: 1\n",
            "*** Num TPU Cores Per Worker: 8\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1069271659006181948)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 4190302933752566319)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -5804962861961968664)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -1152091869502581392)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -1985778295300479180)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -538543547233395006)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -3079606049936719556)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -3532462430369690298)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7413822186867380253)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 1414498527451751799)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, -721696385935440543)\n",
            "Calling model_fn.\n",
            "num_cores_per_replica: 1\n",
            "computation_shape: [1, 1, 1, 1]\n",
            "num_replicas: 8\n",
            "device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "2021-04-21 20:46:11.808682: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "device_list = ['/job:worker/task:0/device:CPU:0']\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "SimdMeshImpl init: Shape[x=1, y=8] LayoutRules{('batch', 'x'), ('embd', 'y'), ('memory_length', 'y')}\n",
            "Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7f17cffeb810>\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Create pnum_tensor\n",
            "Casting <dtype: 'int32'> to float32 for allreduce\n",
            "Casting <dtype: 'int32'> to float32 for allreduce\n",
            "Variable gpt2/h0/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h0/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h1/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h1/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h10/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h10/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h11/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h11/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h12/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h12/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h13/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h13/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h14/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h14/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h15/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h15/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h16/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h16/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h17/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h17/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h18/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h18/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h19/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h19/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h2/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h2/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h20/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h20/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h21/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h21/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h22/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h22/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h23/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h23/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h3/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h3/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h4/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h4/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h5/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h5/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h6/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h6/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h7/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h7/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h8/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h8/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h9/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h9/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/wpe                                                     size 4194304      slice_size 524288       Shape[embed_sequence=2048, embd=2048]                       \n",
            "Variable gpt2/wte                                                     size 102926336    slice_size 12865792     Shape[vocab=50257, embd=2048]                               \n",
            "Variable stacked/gpt2/h0/mlp/conv1d_main/c_fc/bias                    size 65536        slice_size 65536        Shape[stacked=8, intermediate_expanded=8192]                \n",
            "    gpt2/h0/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h1/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h2/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h3/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h4/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h5/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h6/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h7/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h0/norm_1/g                                     size 299008       slice_size 37376        Shape[stacked=146, embd=2048]                               \n",
            "    gpt2/h0/norm_1/g\n",
            "    gpt2/h0/norm_1/b\n",
            "    gpt2/h0/attn/compute_output_bias/o_b\n",
            "    gpt2/h0/norm_2/g\n",
            "    gpt2/h0/norm_2/b\n",
            "    gpt2/h0/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h1/norm_1/g\n",
            "    gpt2/h1/norm_1/b\n",
            "    gpt2/h1/attn/compute_output_bias/o_b\n",
            "    gpt2/h1/norm_2/g\n",
            "    gpt2/h1/norm_2/b\n",
            "    gpt2/h1/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h2/norm_1/g\n",
            "    gpt2/h2/norm_1/b\n",
            "    gpt2/h2/attn/compute_output_bias/o_b\n",
            "    gpt2/h2/norm_2/g\n",
            "    gpt2/h2/norm_2/b\n",
            "    gpt2/h2/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h3/norm_1/g\n",
            "    gpt2/h3/norm_1/b\n",
            "    gpt2/h3/attn/compute_output_bias/o_b\n",
            "    gpt2/h3/norm_2/g\n",
            "    gpt2/h3/norm_2/b\n",
            "    gpt2/h3/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h4/norm_1/g\n",
            "    gpt2/h4/norm_1/b\n",
            "    gpt2/h4/attn/compute_output_bias/o_b\n",
            "    gpt2/h4/norm_2/g\n",
            "    gpt2/h4/norm_2/b\n",
            "    gpt2/h4/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h5/norm_1/g\n",
            "    gpt2/h5/norm_1/b\n",
            "    gpt2/h5/attn/compute_output_bias/o_b\n",
            "    gpt2/h5/norm_2/g\n",
            "    gpt2/h5/norm_2/b\n",
            "    gpt2/h5/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h6/norm_1/g\n",
            "    gpt2/h6/norm_1/b\n",
            "    gpt2/h6/attn/compute_output_bias/o_b\n",
            "    gpt2/h6/norm_2/g\n",
            "    gpt2/h6/norm_2/b\n",
            "    gpt2/h6/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h7/norm_1/g\n",
            "    gpt2/h7/norm_1/b\n",
            "    gpt2/h7/attn/compute_output_bias/o_b\n",
            "    gpt2/h7/norm_2/g\n",
            "    gpt2/h7/norm_2/b\n",
            "    gpt2/h7/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h8/norm_1/g\n",
            "    gpt2/h8/norm_1/b\n",
            "    gpt2/h8/attn/compute_output_bias/o_b\n",
            "    gpt2/h8/norm_2/g\n",
            "    gpt2/h8/norm_2/b\n",
            "    gpt2/h8/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h9/norm_1/g\n",
            "    gpt2/h9/norm_1/b\n",
            "    gpt2/h9/attn/compute_output_bias/o_b\n",
            "    gpt2/h9/norm_2/g\n",
            "    gpt2/h9/norm_2/b\n",
            "    gpt2/h9/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h10/norm_1/g\n",
            "    gpt2/h10/norm_1/b\n",
            "    gpt2/h10/attn/compute_output_bias/o_b\n",
            "    gpt2/h10/norm_2/g\n",
            "    gpt2/h10/norm_2/b\n",
            "    gpt2/h10/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h11/norm_1/g\n",
            "    gpt2/h11/norm_1/b\n",
            "    gpt2/h11/attn/compute_output_bias/o_b\n",
            "    gpt2/h11/norm_2/g\n",
            "    gpt2/h11/norm_2/b\n",
            "    gpt2/h11/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h12/norm_1/g\n",
            "    gpt2/h12/norm_1/b\n",
            "    gpt2/h12/attn/compute_output_bias/o_b\n",
            "    gpt2/h12/norm_2/g\n",
            "    gpt2/h12/norm_2/b\n",
            "    gpt2/h12/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h13/norm_1/g\n",
            "    gpt2/h13/norm_1/b\n",
            "    gpt2/h13/attn/compute_output_bias/o_b\n",
            "    gpt2/h13/norm_2/g\n",
            "    gpt2/h13/norm_2/b\n",
            "    gpt2/h13/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h14/norm_1/g\n",
            "    gpt2/h14/norm_1/b\n",
            "    gpt2/h14/attn/compute_output_bias/o_b\n",
            "    gpt2/h14/norm_2/g\n",
            "    gpt2/h14/norm_2/b\n",
            "    gpt2/h14/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h15/norm_1/g\n",
            "    gpt2/h15/norm_1/b\n",
            "    gpt2/h15/attn/compute_output_bias/o_b\n",
            "    gpt2/h15/norm_2/g\n",
            "    gpt2/h15/norm_2/b\n",
            "    gpt2/h15/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h16/norm_1/g\n",
            "    gpt2/h16/norm_1/b\n",
            "    gpt2/h16/attn/compute_output_bias/o_b\n",
            "    gpt2/h16/norm_2/g\n",
            "    gpt2/h16/norm_2/b\n",
            "    gpt2/h16/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h17/norm_1/g\n",
            "    gpt2/h17/norm_1/b\n",
            "    gpt2/h17/attn/compute_output_bias/o_b\n",
            "    gpt2/h17/norm_2/g\n",
            "    gpt2/h17/norm_2/b\n",
            "    gpt2/h17/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h18/norm_1/g\n",
            "    gpt2/h18/norm_1/b\n",
            "    gpt2/h18/attn/compute_output_bias/o_b\n",
            "    gpt2/h18/norm_2/g\n",
            "    gpt2/h18/norm_2/b\n",
            "    gpt2/h18/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h19/norm_1/g\n",
            "    gpt2/h19/norm_1/b\n",
            "    gpt2/h19/attn/compute_output_bias/o_b\n",
            "    gpt2/h19/norm_2/g\n",
            "    gpt2/h19/norm_2/b\n",
            "    gpt2/h19/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h20/norm_1/g\n",
            "    gpt2/h20/norm_1/b\n",
            "    gpt2/h20/attn/compute_output_bias/o_b\n",
            "    gpt2/h20/norm_2/g\n",
            "    gpt2/h20/norm_2/b\n",
            "    gpt2/h20/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h21/norm_1/g\n",
            "    gpt2/h21/norm_1/b\n",
            "    gpt2/h21/attn/compute_output_bias/o_b\n",
            "    gpt2/h21/norm_2/g\n",
            "    gpt2/h21/norm_2/b\n",
            "    gpt2/h21/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h22/norm_1/g\n",
            "    gpt2/h22/norm_1/b\n",
            "    gpt2/h22/attn/compute_output_bias/o_b\n",
            "    gpt2/h22/norm_2/g\n",
            "    gpt2/h22/norm_2/b\n",
            "    gpt2/h22/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h23/norm_1/g\n",
            "    gpt2/h23/norm_1/b\n",
            "    gpt2/h23/attn/compute_output_bias/o_b\n",
            "    gpt2/h23/norm_2/g\n",
            "    gpt2/h23/norm_2/b\n",
            "    gpt2/h23/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/ln_f/g\n",
            "    gpt2/ln_f/b\n",
            "Variable stacked/gpt2/h16/mlp/conv1d_main/c_fc/bias                   size 65536        slice_size 65536        Shape[stacked=8, intermediate_expanded=8192]                \n",
            "    gpt2/h16/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h17/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h18/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h19/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h20/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h21/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h22/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h23/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h8/mlp/conv1d_main/c_fc/bias                    size 65536        slice_size 65536        Shape[stacked=8, intermediate_expanded=8192]                \n",
            "    gpt2/h8/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h9/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h10/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h11/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h12/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h13/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h14/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h15/mlp/conv1d_main/c_fc/bias\n",
            "Trainable Variables            count: 150     Total size: 1315575808       Total slice_size: 164619008      \n",
            "All Variables                  count: 150     Total size: 1315575808       Total slice_size: 164619008      \n",
            "Counters:\n",
            "allconcat: 3.28e+04\n",
            " allconcat/0: 3.28e+04\n",
            "  allconcat/0/reshape_op: 3.28e+04\n",
            "allreduce: 6.86e+09\n",
            " allreduce/[0]: 8\n",
            "  allreduce/[0]/reduce_op: 8\n",
            " allreduce/[1]: 6.86e+09\n",
            "  allreduce/[1]/einsum_op: 6.85e+09\n",
            "  allreduce/[1]/reduce_op: 7.9e+06\n",
            "einsum: 3.95e+12\n",
            "einsum_unique: 3.22e+12\n",
            "output: 7.76e+10\n",
            " output/AddOperation: 1.46e+10\n",
            " output/BinaryOpWithBroadcasting: 5.08e+08\n",
            " output/BroadcastOperation: 8.14e+08\n",
            " output/ConcatOperation: 1.61e+09\n",
            " output/Constant: 1.97e+05\n",
            " output/EinsumOperation: 2.08e+10\n",
            " output/ImportOperation: 3.3e+04\n",
            " output/OneHotOperation: 8.57e+08\n",
            " output/RangeOperation: 2.48e+05\n",
            " output/ReduceOperation: 1.42e+07\n",
            " output/ReshapeOperation: 5.74e+09\n",
            " output/ScalarAddOperation: 3.22e+09\n",
            " output/ScalarMultiplyOperation: 9.87e+09\n",
            " output/ShiftOperation: 8.06e+08\n",
            " output/SlicewiseOperation: 1.35e+10\n",
            " output/StackedVariable: 1.87e+06\n",
            " output/StopGradient: 2.42e+09\n",
            " output/UnstackOperation: 1.87e+06\n",
            " output/Variable: 1.32e+09\n",
            " output/WhileLoopOperation: 1.61e+09\n",
            "output_unique: 1.96e+10\n",
            " output_unique/AddOperation: 4.66e+09\n",
            " output_unique/BinaryOpWithBroadcasting: 6.71e+07\n",
            " output_unique/BroadcastOperation: 8.14e+08\n",
            " output_unique/ConcatOperation: 2.01e+08\n",
            " output_unique/Constant: 2.46e+04\n",
            " output_unique/EinsumOperation: 3.84e+09\n",
            " output_unique/ImportOperation: 4.12e+03\n",
            " output_unique/OneHotOperation: 1.07e+08\n",
            " output_unique/RangeOperation: 3.28e+04\n",
            " output_unique/ReduceOperation: 1.77e+06\n",
            " output_unique/ReshapeOperation: 8.05e+08\n",
            " output_unique/ScalarAddOperation: 4.03e+08\n",
            " output_unique/ScalarMultiplyOperation: 1.33e+09\n",
            " output_unique/ShiftOperation: 1.01e+08\n",
            " output_unique/SlicewiseOperation: 4.73e+09\n",
            " output_unique/StackedVariable: 4.96e+05\n",
            " output_unique/StopGradient: 1.01e+09\n",
            " output_unique/UnstackOperation: 4.96e+05\n",
            " output_unique/Variable: 1.32e+09\n",
            " output_unique/WhileLoopOperation: 2.01e+08\n",
            "variables: 1.32e+09\n",
            " variables/trainable: 1.32e+09\n",
            "Done calling model_fn.\n",
            "TPU job name worker\n",
            "Graph was finalized.\n",
            "Restoring parameters from gs://bigbird-freefly/neo_gpt3/gpt3_XL/the-eye.eu/public/AI/gptneo-release/all_balanced_train_b/model.ckpt-736911\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:840: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "Starting infeed thread controller.\n",
            "Starting outfeed thread controller.\n",
            "Initialized dataset iterators in 0 seconds\n",
            "Before copy master to slices.\n",
            "Done with copy master to slices.\n",
            "Enqueue next (1) batch(es) of data to infeed.\n",
            "Dequeue next (1) batch(es) of data from outfeed.\n",
            "Outfeed finished for iteration (0, 0)\n",
            "======================================== SAMPLE 0 ========================================\n",
            "\n",
            "This is a review:\n",
            "Enough similarities to Gymkata and Howie Long's Firestorm that my fingernails instinctively crawled towards my long-suffering eyeballs.\n",
            "sentiment is:neutral\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Enqueue next (1) batch(es) of data to infeed.\n",
            "Dequeue next (1) batch(es) of data from outfeed.\n",
            "Stop infeed thread controller\n",
            "Shutting down InfeedController thread.\n",
            "InfeedController received shutdown signal, stopping.\n",
            "Infeed thread finished, shutting down.\n",
            "infeed marked as finished\n",
            "Stop output thread controller\n",
            "Shutting down OutfeedController thread.\n",
            "OutfeedController received shutdown signal, stopping.\n",
            "Outfeed thread finished, shutting down.\n",
            "outfeed marked as finished\n",
            "Shutdown TPU system.\n",
            "prediction_loop marked as finished\n",
            "prediction_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}