{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offchart-star-GPT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riccardo247/star-GPT/blob/main/offchart_star_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz3CbT3miEQF"
      },
      "source": [
        "**Introduction:**\n",
        "This a notebook showing how to generate sentiment with an all in text approach.\n",
        "The model used is GPT-neo. This specific version was pretrained on the pile and on c4 (colossal dataset) for 700k steps. Further pretrained on yelp reviews and imdb dataset.\n",
        "For this specific task the model was fine tuned as well on reviews dataset.\n",
        "\n",
        "**Example:** Given a reviews or text in input the model wil complete with sentiment negative/positive/neutral.\n",
        "\n",
        "**Review:**\n",
        "Succeeds in providing a disquiet world the long-dreaded completion of the Police Academy series . sentiment is:...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tii5zEBFydn"
      },
      "source": [
        "**Download all needd packages**\n",
        "\n",
        "\n",
        "> package GPTNeo is needed with all the requirements\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qe-S3h9AKpd",
        "outputId": "df8151d8-617b-4d99-b958-4069a4f4f960"
      },
      "source": [
        "#@title Setup\n",
        "%tensorflow_version 2.x\n",
        "!git clone https://github.com/EleutherAI/GPTNeo\n",
        "%cd GPTNeo\n",
        "!pip3 install -q -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GPTNeo'...\n",
            "remote: Enumerating objects: 3747, done.\u001b[K\n",
            "remote: Counting objects: 100% (279/279), done.\u001b[K\n",
            "remote: Compressing objects: 100% (131/131), done.\u001b[K\n",
            "remote: Total 3747 (delta 167), reused 255 (delta 147), pack-reused 3468\u001b[K\n",
            "Receiving objects: 100% (3747/3747), 1.43 MiB | 11.28 MiB/s, done.\n",
            "Resolving deltas: 100% (2167/2167), done.\n",
            "/content/GPTNeo\n",
            "\u001b[K     |████████████████████████████████| 368kB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 14.2MB 297kB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 43.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 394.7MB 39kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 45.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 50.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 45.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 47.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 184kB 58.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 47.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 57.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 8.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 39.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 286kB 56.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \n",
            "\u001b[?25h  Building wheel for tpunicorn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ring (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wirerope (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KgpQVa6F2K5"
      },
      "source": [
        "**Gopogle authentication** could be needed to access the Google bucket storing weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pofjb285ESuf"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxAK-TCzGJeS"
      },
      "source": [
        "**Configuration**\n",
        "\n",
        "\n",
        "> Remember to change running time to TPU\n",
        "\n",
        "\n",
        "\n",
        "> The following are configurations files for the model and data. They do not need to be modified\n",
        "\n",
        "\n",
        "> It is using GPT2 tokenizer\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMCzeJG8Ap1P",
        "outputId": "6c6607ed-717a-4c9b-9dc1-41e4841506e9"
      },
      "source": [
        "%%writefile configs/GPT3_XL_bakeoff.json\n",
        "\n",
        "{\n",
        "    \"n_head\": 16,\n",
        "    \"n_vocab\": 50257,\n",
        "    \"embed_dropout\": 0,\n",
        "    \"lr\": 0.0002,\n",
        "    \"lr_decay\": \"cosine\",\n",
        "    \"warmup_steps\": 3000,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.95,\n",
        "    \"epsilon\": 1e-8,\n",
        "    \"opt_name\": \"adam\",\n",
        "    \"weight_decay\": 0,\n",
        "    \"train_batch_size\": 1,\n",
        "    \"attn_dropout\": 0,\n",
        "    \"train_steps\": 744136,\n",
        "    \"lr_decay_end\" : 300000,\n",
        "    \"eval_steps\": 20,\n",
        "    \"predict_steps\":1,\n",
        "    \"res_dropout\": 0,\n",
        "    \"eval_batch_size\": 1,\n",
        "    \"predict_batch_size\": 1,\n",
        "    \"iterations\": 100,\n",
        "    \"n_embd\": 2048,\n",
        "    \"datasets\": [[\"sst3_shortbake_label_pred\"]], \n",
        "    \"model_path\": \"gs://bigbird-freefly/neo_gpt3/gpt3_XL/the-eye.eu/public/AI/gptneo-release/all_balanced_train_b\",\n",
        "    \"n_ctx\": 2048,\n",
        "    \"n_layer\": 24,\n",
        "    \"scale_by_depth\": true,\n",
        "    \"scale_by_in\": false,\n",
        "    \"attention_types\" :  [[[\"global\", \"local\"],12]],\n",
        "    \"mesh_shape\": \"x:1,y:8\",\n",
        "    \"layout\" : \"batch:x,memory_length:y,embd:y\",\n",
        "    \"activation_function\": \"gelu\",\n",
        "    \"recompute_grad\": true,\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"tokens_per_mb_per_replica\": 2048,\n",
        "    \"precision\": \"bfloat16\",\n",
        "    \"padding_id\" : 50257,\n",
        "    \"eos_id\" : 50256\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting configs/GPT3_XL_bakeoff.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQESZkbiDjBZ",
        "outputId": "b5aa567f-a962-4295-f7a1-182021463163"
      },
      "source": [
        "%%writefile configs/dataset_configs/sst3_shortbake_label_pred.json\n",
        "{\n",
        "    \"n_vocab\": 50257,\n",
        "    \"path\": \"gs://bigbird-freefly/sst3/bakeoff_entry//sst*.tfrecords\", \n",
        "    \"eval_path\": \"gs://bigbird-freefly/sst3/dev//imbd*.tfrecords\",\n",
        "    \"tokenizer_is_pretrained\": true,\n",
        "    \"tokenizer_path\": \"gpt2\",\n",
        "    \"eos_id\" : 50256,\n",
        "    \"padding_id\": 50257\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting configs/dataset_configs/sst3_shortbake_label_pred.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruZbvBhRIcf8",
        "outputId": "b86e045f-5f48-47d5-a109-73a7cd1bcadd"
      },
      "source": [
        "!gsutil cp gs://bigbird-freefly/sst3/files/sample.py ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bigbird-freefly/sst3/files/sample.py...\n",
            "- [1 files][  9.1 KiB/  9.1 KiB]                                                \n",
            "Operation completed over 1 objects/9.1 KiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vzWJi_vGZFB"
      },
      "source": [
        "**Review examples**\n",
        "\n",
        "\n",
        "> The following code cell writes to a txt a review example.  Iprovide 3 examples for positive, neutral, negative sentiment. The text should be formatted in 3 lines as:\n",
        "\n",
        "\n",
        "> This is the review:\n",
        "...put the text of the review in this line...\n",
        "sentiment is:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkSfy4FUJmdZ",
        "outputId": "a2e968d2-eee2-479d-c4a5-f7b1d2b54faf"
      },
      "source": [
        "%%writefile review_11820-00001.txt\n",
        "This is a review:\n",
        "Enough similarities to Gymkata and Howie Long 's Firestorm that my fingernails instinctively crawled towards my long-suffering eyeballs .\n",
        "sentiment is:"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting review_11820-00001.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_K5dTBZA2Er",
        "outputId": "88b716f3-ff8c-403f-8b27-856d7f0eff39"
      },
      "source": [
        "%%writefile review_11830-00001.txt\n",
        "This is a review:\n",
        "While the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun .\n",
        "sentiment is:"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting review_11830-00001.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_JFAdT-OD3v"
      },
      "source": [
        "%%writefile review_11821-00001.txt\n",
        "This is a review:\n",
        "Succeeds in providing a disquiet world the long-dreaded completion of the Police Academy series .\n",
        "sentiment is:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES59Rca6KiTU"
      },
      "source": [
        "**Sentimen prediction:**\n",
        "The following code is running prediction for the txt file and output the result to the cell. Change the filename or create a new one\n",
        "\n",
        ">The first time it will take 10 minutes because it has to download the model weights. Second time will be 1-2 minutes. This notebook is just to show how it works but it is very slow!\n",
        "Output will be the input ending in sentiment like this:\n",
        "This is a review:\n",
        "While the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun .\n",
        "sentiment is: **negative**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x2Qg1OVCdWi",
        "outputId": "cc9befe7-fce4-4498-e341-39819d6d05b3"
      },
      "source": [
        "review_file ='review_11820-00001.txt'\n",
        "!python3 main.py --model GPT3_XL_bakeoff --steps_per_checkpoint 100 --tpu colab --predict --prompt $review_file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-21 20:45:26.644600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Current step 736911\n",
            "Saving config to gs://bigbird-freefly/neo_gpt3/gpt3_XL/the-eye.eu/public/AI/gptneo-release/all_balanced_train_b\n",
            "2021-04-21 20:45:35.333094: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-21 20:45:35.334145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-21 20:45:35.346532: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-04-21 20:45:35.346619: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (945b4dd6c79b): /proc/driver/nvidia/version does not exist\n",
            "2021-04-21 20:45:37.541947: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
            "Done!\n",
            "params = defaultdict(<function fetch_model_params.<locals>.<lambda> at 0x7f17d57784d0>, {'n_head': 16, 'n_vocab': 50257, 'embed_dropout': 0, 'lr': 0.0002, 'lr_decay': 'cosine', 'warmup_steps': 3000, 'beta1': 0.9, 'beta2': 0.95, 'epsilon': 1e-08, 'opt_name': 'adam', 'weight_decay': 0, 'train_batch_size': 1, 'attn_dropout': 0, 'train_steps': 744136, 'lr_decay_end': 300000, 'eval_steps': 20, 'predict_steps': 1, 'res_dropout': 0, 'eval_batch_size': 1, 'predict_batch_size': 1, 'iterations': 100, 'n_embd': 2048, 'datasets': [['sst3_shortbake_label_pred']], 'model_path': 'gs://bigbird-freefly/neo_gpt3/gpt3_XL/the-eye.eu/public/AI/gptneo-release/all_balanced_train_b', 'n_ctx': 2048, 'n_layer': 24, 'scale_by_depth': True, 'scale_by_in': False, 'attention_types': ['global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local'], 'mesh_shape': 'x:1,y:8', 'layout': 'batch:x,memory_length:y,embd:y', 'activation_function': 'gelu', 'recompute_grad': True, 'gradient_clipping': 1.0, 'tokens_per_mb_per_replica': 2048, 'precision': 'bfloat16', 'padding_id': 50257, 'eos_id': 50256, 'dataset_configs': {'sst3_shortbake_label_pred': {'n_vocab': 50257, 'path': 'gs://bigbird-freefly/sst3/bakeoff_entry//sst*.tfrecords', 'eval_path': 'gs://bigbird-freefly/sst3/dev//imbd*.tfrecords', 'tokenizer_is_pretrained': True, 'tokenizer_path': 'gpt2', 'eos_id': 50256, 'padding_id': 50257}}, 'mlm_training': False, 'causal': True, 'num_cores': 8, 'auto_layout': False, 'auto_layout_and_mesh_shape': False, 'use_tpu': True, 'gpu_ids': ['device:GPU:0'], 'steps_per_checkpoint': 100, 'predict': True, 'model': 'GPT', 'export': False, 'sampling_use_entmax': False, 'moe_layers': None, 'slow_sampling': False})\n",
            "Using config: {'_model_dir': 'gs://bigbird-freefly/neo_gpt3/gpt3_XL/the-eye.eu/public/AI/gptneo-release/all_balanced_train_b', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.113.140.66:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.113.140.66:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.113.140.66:8470', '_evaluation_master': 'grpc://10.113.140.66:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7f17d57619d0>}\n",
            "_TPUContext: eval_on_tpu True\n",
            "Predictions generated\n",
            "Querying Tensorflow master (grpc://10.113.140.66:8470) for TPU system metadata.\n",
            "2021-04-21 20:45:39.805784: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "Initializing TPU system (master: grpc://10.113.140.66:8470) to fetch topology for model parallelism. This might take a while.\n",
            "Found TPU system:\n",
            "*** Num TPU Cores: 8\n",
            "*** Num TPU Workers: 1\n",
            "*** Num TPU Cores Per Worker: 8\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1069271659006181948)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 4190302933752566319)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -5804962861961968664)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -1152091869502581392)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -1985778295300479180)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -538543547233395006)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -3079606049936719556)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -3532462430369690298)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7413822186867380253)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 1414498527451751799)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, -721696385935440543)\n",
            "Calling model_fn.\n",
            "num_cores_per_replica: 1\n",
            "computation_shape: [1, 1, 1, 1]\n",
            "num_replicas: 8\n",
            "device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "2021-04-21 20:46:11.808682: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "device_list = ['/job:worker/task:0/device:CPU:0']\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "SimdMeshImpl init: Shape[x=1, y=8] LayoutRules{('batch', 'x'), ('embd', 'y'), ('memory_length', 'y')}\n",
            "Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7f17cffeb810>\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Create pnum_tensor\n",
            "Casting <dtype: 'int32'> to float32 for allreduce\n",
            "Casting <dtype: 'int32'> to float32 for allreduce\n",
            "Variable gpt2/h0/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h0/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h1/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h1/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h10/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h10/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h11/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h11/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h12/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h12/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h13/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h13/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h14/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h14/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h15/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h15/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h16/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h16/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h17/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h17/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h18/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h18/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h19/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h19/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h2/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h2/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h20/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h20/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h21/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h21/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h22/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h22/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h23/attn/k                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/attn/o                                              size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h23/attn/q                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/attn/v                                              size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_fc/kernel                         size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_proj/kernel                       size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h3/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h3/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h4/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h4/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h5/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h5/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h6/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h6/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h7/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h7/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h8/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h8/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/h9/attn/k                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/attn/o                                               size 4194304      slice_size 524288       Shape[heads=2048, embd=2048]                                \n",
            "Variable gpt2/h9/attn/q                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/attn/v                                               size 4194304      slice_size 524288       Shape[embd=2048, heads=2048]                                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_fc/kernel                          size 16777216     slice_size 2097152      Shape[embd=2048, intermediate_expanded=8192]                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_proj/kernel                        size 16777216     slice_size 2097152      Shape[intermediate_expanded=8192, embd=2048]                \n",
            "Variable gpt2/wpe                                                     size 4194304      slice_size 524288       Shape[embed_sequence=2048, embd=2048]                       \n",
            "Variable gpt2/wte                                                     size 102926336    slice_size 12865792     Shape[vocab=50257, embd=2048]                               \n",
            "Variable stacked/gpt2/h0/mlp/conv1d_main/c_fc/bias                    size 65536        slice_size 65536        Shape[stacked=8, intermediate_expanded=8192]                \n",
            "    gpt2/h0/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h1/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h2/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h3/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h4/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h5/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h6/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h7/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h0/norm_1/g                                     size 299008       slice_size 37376        Shape[stacked=146, embd=2048]                               \n",
            "    gpt2/h0/norm_1/g\n",
            "    gpt2/h0/norm_1/b\n",
            "    gpt2/h0/attn/compute_output_bias/o_b\n",
            "    gpt2/h0/norm_2/g\n",
            "    gpt2/h0/norm_2/b\n",
            "    gpt2/h0/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h1/norm_1/g\n",
            "    gpt2/h1/norm_1/b\n",
            "    gpt2/h1/attn/compute_output_bias/o_b\n",
            "    gpt2/h1/norm_2/g\n",
            "    gpt2/h1/norm_2/b\n",
            "    gpt2/h1/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h2/norm_1/g\n",
            "    gpt2/h2/norm_1/b\n",
            "    gpt2/h2/attn/compute_output_bias/o_b\n",
            "    gpt2/h2/norm_2/g\n",
            "    gpt2/h2/norm_2/b\n",
            "    gpt2/h2/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h3/norm_1/g\n",
            "    gpt2/h3/norm_1/b\n",
            "    gpt2/h3/attn/compute_output_bias/o_b\n",
            "    gpt2/h3/norm_2/g\n",
            "    gpt2/h3/norm_2/b\n",
            "    gpt2/h3/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h4/norm_1/g\n",
            "    gpt2/h4/norm_1/b\n",
            "    gpt2/h4/attn/compute_output_bias/o_b\n",
            "    gpt2/h4/norm_2/g\n",
            "    gpt2/h4/norm_2/b\n",
            "    gpt2/h4/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h5/norm_1/g\n",
            "    gpt2/h5/norm_1/b\n",
            "    gpt2/h5/attn/compute_output_bias/o_b\n",
            "    gpt2/h5/norm_2/g\n",
            "    gpt2/h5/norm_2/b\n",
            "    gpt2/h5/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h6/norm_1/g\n",
            "    gpt2/h6/norm_1/b\n",
            "    gpt2/h6/attn/compute_output_bias/o_b\n",
            "    gpt2/h6/norm_2/g\n",
            "    gpt2/h6/norm_2/b\n",
            "    gpt2/h6/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h7/norm_1/g\n",
            "    gpt2/h7/norm_1/b\n",
            "    gpt2/h7/attn/compute_output_bias/o_b\n",
            "    gpt2/h7/norm_2/g\n",
            "    gpt2/h7/norm_2/b\n",
            "    gpt2/h7/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h8/norm_1/g\n",
            "    gpt2/h8/norm_1/b\n",
            "    gpt2/h8/attn/compute_output_bias/o_b\n",
            "    gpt2/h8/norm_2/g\n",
            "    gpt2/h8/norm_2/b\n",
            "    gpt2/h8/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h9/norm_1/g\n",
            "    gpt2/h9/norm_1/b\n",
            "    gpt2/h9/attn/compute_output_bias/o_b\n",
            "    gpt2/h9/norm_2/g\n",
            "    gpt2/h9/norm_2/b\n",
            "    gpt2/h9/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h10/norm_1/g\n",
            "    gpt2/h10/norm_1/b\n",
            "    gpt2/h10/attn/compute_output_bias/o_b\n",
            "    gpt2/h10/norm_2/g\n",
            "    gpt2/h10/norm_2/b\n",
            "    gpt2/h10/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h11/norm_1/g\n",
            "    gpt2/h11/norm_1/b\n",
            "    gpt2/h11/attn/compute_output_bias/o_b\n",
            "    gpt2/h11/norm_2/g\n",
            "    gpt2/h11/norm_2/b\n",
            "    gpt2/h11/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h12/norm_1/g\n",
            "    gpt2/h12/norm_1/b\n",
            "    gpt2/h12/attn/compute_output_bias/o_b\n",
            "    gpt2/h12/norm_2/g\n",
            "    gpt2/h12/norm_2/b\n",
            "    gpt2/h12/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h13/norm_1/g\n",
            "    gpt2/h13/norm_1/b\n",
            "    gpt2/h13/attn/compute_output_bias/o_b\n",
            "    gpt2/h13/norm_2/g\n",
            "    gpt2/h13/norm_2/b\n",
            "    gpt2/h13/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h14/norm_1/g\n",
            "    gpt2/h14/norm_1/b\n",
            "    gpt2/h14/attn/compute_output_bias/o_b\n",
            "    gpt2/h14/norm_2/g\n",
            "    gpt2/h14/norm_2/b\n",
            "    gpt2/h14/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h15/norm_1/g\n",
            "    gpt2/h15/norm_1/b\n",
            "    gpt2/h15/attn/compute_output_bias/o_b\n",
            "    gpt2/h15/norm_2/g\n",
            "    gpt2/h15/norm_2/b\n",
            "    gpt2/h15/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h16/norm_1/g\n",
            "    gpt2/h16/norm_1/b\n",
            "    gpt2/h16/attn/compute_output_bias/o_b\n",
            "    gpt2/h16/norm_2/g\n",
            "    gpt2/h16/norm_2/b\n",
            "    gpt2/h16/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h17/norm_1/g\n",
            "    gpt2/h17/norm_1/b\n",
            "    gpt2/h17/attn/compute_output_bias/o_b\n",
            "    gpt2/h17/norm_2/g\n",
            "    gpt2/h17/norm_2/b\n",
            "    gpt2/h17/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h18/norm_1/g\n",
            "    gpt2/h18/norm_1/b\n",
            "    gpt2/h18/attn/compute_output_bias/o_b\n",
            "    gpt2/h18/norm_2/g\n",
            "    gpt2/h18/norm_2/b\n",
            "    gpt2/h18/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h19/norm_1/g\n",
            "    gpt2/h19/norm_1/b\n",
            "    gpt2/h19/attn/compute_output_bias/o_b\n",
            "    gpt2/h19/norm_2/g\n",
            "    gpt2/h19/norm_2/b\n",
            "    gpt2/h19/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h20/norm_1/g\n",
            "    gpt2/h20/norm_1/b\n",
            "    gpt2/h20/attn/compute_output_bias/o_b\n",
            "    gpt2/h20/norm_2/g\n",
            "    gpt2/h20/norm_2/b\n",
            "    gpt2/h20/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h21/norm_1/g\n",
            "    gpt2/h21/norm_1/b\n",
            "    gpt2/h21/attn/compute_output_bias/o_b\n",
            "    gpt2/h21/norm_2/g\n",
            "    gpt2/h21/norm_2/b\n",
            "    gpt2/h21/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h22/norm_1/g\n",
            "    gpt2/h22/norm_1/b\n",
            "    gpt2/h22/attn/compute_output_bias/o_b\n",
            "    gpt2/h22/norm_2/g\n",
            "    gpt2/h22/norm_2/b\n",
            "    gpt2/h22/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h23/norm_1/g\n",
            "    gpt2/h23/norm_1/b\n",
            "    gpt2/h23/attn/compute_output_bias/o_b\n",
            "    gpt2/h23/norm_2/g\n",
            "    gpt2/h23/norm_2/b\n",
            "    gpt2/h23/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/ln_f/g\n",
            "    gpt2/ln_f/b\n",
            "Variable stacked/gpt2/h16/mlp/conv1d_main/c_fc/bias                   size 65536        slice_size 65536        Shape[stacked=8, intermediate_expanded=8192]                \n",
            "    gpt2/h16/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h17/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h18/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h19/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h20/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h21/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h22/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h23/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h8/mlp/conv1d_main/c_fc/bias                    size 65536        slice_size 65536        Shape[stacked=8, intermediate_expanded=8192]                \n",
            "    gpt2/h8/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h9/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h10/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h11/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h12/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h13/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h14/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h15/mlp/conv1d_main/c_fc/bias\n",
            "Trainable Variables            count: 150     Total size: 1315575808       Total slice_size: 164619008      \n",
            "All Variables                  count: 150     Total size: 1315575808       Total slice_size: 164619008      \n",
            "Counters:\n",
            "allconcat: 3.28e+04\n",
            " allconcat/0: 3.28e+04\n",
            "  allconcat/0/reshape_op: 3.28e+04\n",
            "allreduce: 6.86e+09\n",
            " allreduce/[0]: 8\n",
            "  allreduce/[0]/reduce_op: 8\n",
            " allreduce/[1]: 6.86e+09\n",
            "  allreduce/[1]/einsum_op: 6.85e+09\n",
            "  allreduce/[1]/reduce_op: 7.9e+06\n",
            "einsum: 3.95e+12\n",
            "einsum_unique: 3.22e+12\n",
            "output: 7.76e+10\n",
            " output/AddOperation: 1.46e+10\n",
            " output/BinaryOpWithBroadcasting: 5.08e+08\n",
            " output/BroadcastOperation: 8.14e+08\n",
            " output/ConcatOperation: 1.61e+09\n",
            " output/Constant: 1.97e+05\n",
            " output/EinsumOperation: 2.08e+10\n",
            " output/ImportOperation: 3.3e+04\n",
            " output/OneHotOperation: 8.57e+08\n",
            " output/RangeOperation: 2.48e+05\n",
            " output/ReduceOperation: 1.42e+07\n",
            " output/ReshapeOperation: 5.74e+09\n",
            " output/ScalarAddOperation: 3.22e+09\n",
            " output/ScalarMultiplyOperation: 9.87e+09\n",
            " output/ShiftOperation: 8.06e+08\n",
            " output/SlicewiseOperation: 1.35e+10\n",
            " output/StackedVariable: 1.87e+06\n",
            " output/StopGradient: 2.42e+09\n",
            " output/UnstackOperation: 1.87e+06\n",
            " output/Variable: 1.32e+09\n",
            " output/WhileLoopOperation: 1.61e+09\n",
            "output_unique: 1.96e+10\n",
            " output_unique/AddOperation: 4.66e+09\n",
            " output_unique/BinaryOpWithBroadcasting: 6.71e+07\n",
            " output_unique/BroadcastOperation: 8.14e+08\n",
            " output_unique/ConcatOperation: 2.01e+08\n",
            " output_unique/Constant: 2.46e+04\n",
            " output_unique/EinsumOperation: 3.84e+09\n",
            " output_unique/ImportOperation: 4.12e+03\n",
            " output_unique/OneHotOperation: 1.07e+08\n",
            " output_unique/RangeOperation: 3.28e+04\n",
            " output_unique/ReduceOperation: 1.77e+06\n",
            " output_unique/ReshapeOperation: 8.05e+08\n",
            " output_unique/ScalarAddOperation: 4.03e+08\n",
            " output_unique/ScalarMultiplyOperation: 1.33e+09\n",
            " output_unique/ShiftOperation: 1.01e+08\n",
            " output_unique/SlicewiseOperation: 4.73e+09\n",
            " output_unique/StackedVariable: 4.96e+05\n",
            " output_unique/StopGradient: 1.01e+09\n",
            " output_unique/UnstackOperation: 4.96e+05\n",
            " output_unique/Variable: 1.32e+09\n",
            " output_unique/WhileLoopOperation: 2.01e+08\n",
            "variables: 1.32e+09\n",
            " variables/trainable: 1.32e+09\n",
            "Done calling model_fn.\n",
            "TPU job name worker\n",
            "Graph was finalized.\n",
            "Restoring parameters from gs://bigbird-freefly/neo_gpt3/gpt3_XL/the-eye.eu/public/AI/gptneo-release/all_balanced_train_b/model.ckpt-736911\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:840: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "Starting infeed thread controller.\n",
            "Starting outfeed thread controller.\n",
            "Initialized dataset iterators in 0 seconds\n",
            "Before copy master to slices.\n",
            "Done with copy master to slices.\n",
            "Enqueue next (1) batch(es) of data to infeed.\n",
            "Dequeue next (1) batch(es) of data from outfeed.\n",
            "Outfeed finished for iteration (0, 0)\n",
            "======================================== SAMPLE 0 ========================================\n",
            "\n",
            "This is a review:\n",
            "Enough similarities to Gymkata and Howie Long's Firestorm that my fingernails instinctively crawled towards my long-suffering eyeballs.\n",
            "sentiment is:neutral\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Enqueue next (1) batch(es) of data to infeed.\n",
            "Dequeue next (1) batch(es) of data from outfeed.\n",
            "Stop infeed thread controller\n",
            "Shutting down InfeedController thread.\n",
            "InfeedController received shutdown signal, stopping.\n",
            "Infeed thread finished, shutting down.\n",
            "infeed marked as finished\n",
            "Stop output thread controller\n",
            "Shutting down OutfeedController thread.\n",
            "OutfeedController received shutdown signal, stopping.\n",
            "Outfeed thread finished, shutting down.\n",
            "outfeed marked as finished\n",
            "Shutdown TPU system.\n",
            "prediction_loop marked as finished\n",
            "prediction_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0YnbBtXlhPA"
      },
      "source": [
        "**Run full test data prediction**\n",
        "\n",
        "> All the test set saved in a tfrecord file gs://bigbird-freefly/sst3/bakeoff_entry//sst*.tfrecords\n",
        "\n",
        "\n",
        ">Prediction result will be saved in /test_pred/test_pred_full.jsonl. Running time is about 40 minutes\n",
        "\n",
        "\n",
        "> TODO: add here match.py to match results with original ids and compute f1\n",
        "\n",
        "\n",
        "\n",
        "> Som parameters are still hard-coded in the code. For example sampling temperature is set to 0. I overwrite few files here before execution\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ8KQolPnaEx",
        "outputId": "d9cca0bd-a063-44ee-91a2-348c8859b3ee"
      },
      "source": [
        "!mkdir test_pred\n",
        "!gsutil cp gs://bigbird-freefly/sst3/files/main_predict.py ./\n",
        "!gsutil cp gs://bigbird-freefly/sst3/files/inputs.py ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bigbird-freefly/sst3/files/main_predict.py...\n",
            "- [1 files][ 11.3 KiB/ 11.3 KiB]                                                \n",
            "Operation completed over 1 objects/11.3 KiB.                                     \n",
            "Copying gs://bigbird-freefly/sst3/files/inputs.py...\n",
            "/ [1 files][ 21.7 KiB/ 21.7 KiB]                                                \n",
            "Operation completed over 1 objects/21.7 KiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7kO5P3gpGVR",
        "outputId": "ebcea7e0-9e37-4755-9583-b090896c101f"
      },
      "source": [
        "%%writefile configs/GPT3_XL_bakeoff.json\n",
        "\n",
        "{\n",
        "    \"n_head\": 16,\n",
        "    \"n_vocab\": 50257,\n",
        "    \"embed_dropout\": 0,\n",
        "    \"lr\": 0.0002,\n",
        "    \"lr_decay\": \"cosine\",\n",
        "    \"warmup_steps\": 3000,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.95,\n",
        "    \"epsilon\": 1e-8,\n",
        "    \"opt_name\": \"adam\",\n",
        "    \"weight_decay\": 0,\n",
        "    \"train_batch_size\": 8,\n",
        "    \"attn_dropout\": 0,\n",
        "    \"train_steps\": 744136,\n",
        "    \"lr_decay_end\" : 300000,\n",
        "    \"eval_steps\": 20,\n",
        "    \"predict_steps\":1,\n",
        "    \"res_dropout\": 0,\n",
        "    \"eval_batch_size\": 8,\n",
        "    \"predict_batch_size\": 8,\n",
        "    \"iterations\": 100,\n",
        "    \"n_embd\": 2048,\n",
        "    \"datasets\": [[\"sst3_shortbake_label_pred\"]], \n",
        "    \"model_path\": \"gs://bigbird-freefly/neo_gpt3/gpt3_XL/the-eye.eu/public/AI/gptneo-release/all_balanced_train_b\",\n",
        "    \"n_ctx\": 2048,\n",
        "    \"n_layer\": 24,\n",
        "    \"scale_by_depth\": true,\n",
        "    \"scale_by_in\": false,\n",
        "    \"attention_types\" :  [[[\"global\", \"local\"],12]],\n",
        "    \"mesh_shape\": \"x:2,y:4\",\n",
        "    \"layout\" : \"batch:x,memory_length:y,embd:y\",\n",
        "    \"activation_function\": \"gelu\",\n",
        "    \"recompute_grad\": true,\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"tokens_per_mb_per_replica\": 2048,\n",
        "    \"precision\": \"bfloat16\",\n",
        "    \"padding_id\" : 50257,\n",
        "    \"eos_id\" : 50256\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting configs/GPT3_XL_bakeoff.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7UPhtrZlf24"
      },
      "source": [
        "!python3 main_predict.py --predict --steps_per_checkpoint 200 --tpu colab --model GPT3_XL_bakeoff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIJrbZw9tI6R",
        "outputId": "f9fb8d4d-fa07-4034-9271-227f34a96789"
      },
      "source": [
        "!head ./test_pred/test_pred.jsonl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"text\": \"The host would not seat them, said they were pretty booked, but offered them a table in the bar.\", \"stars\": \"neutral\"}\n",
            "{\"text\": \"The film is just a big, gorgeous, mind-blowing, breath-taking mess.\", \"stars\": \"neutral\"}\n",
            "{\"text\": \"With a story inspired by the tumultuous surroundings of Los Angeles, where feelings of marginalization loom for every dreamer with a burst bubble, The Dogwalker has a few characters and ideas, but it never manages to put them on the same path.\", \"stars\": \"neutral\"}\n",
            "{\"text\": \"This was our fourth visit to Blue Bird.\", \"stars\": \"neutral\"}\n",
            "{\"text\": \"Apparently there are more rooms (downstairs bar with sports on tv) then we saw.\", \"stars\": \"neutral\"}\n",
            "{\"text\": \"A grim, flat and boring werewolf movie that refuses to develop an energy level.\", \"stars\": \"negative\"}\n",
            "{\"text\": \"One of those movies where you walk out of the theater not feeling cheated exactly, but feeling pandered to, which, in the end, might be all the more infuriating.\", \"stars\": \"negative\"}\n",
            "{\"text\": \"The Weight of Water uses water as a metaphor for subconscious desire, but this leaky script barely stays afloat.\", \"stars\": \"negative\"}\n",
            "{\"text\": \"Go to anyone of them.\", \"stars\": \"neutral\"}\n",
            "{\"text\": \"It is not inexpensive for dinner, drinks and dessert -- so just be aware of that.\", \"stars\": \"negative\"}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}